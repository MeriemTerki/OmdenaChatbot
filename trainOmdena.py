# -*- coding: utf-8 -*-
"""chatbotusingtransformers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xtV9qK_ENfHNIqaUHn3pcJQ6bSNA1dB4
"""

import pandas as pd
import json

with open("intents .json") as intents:
  data1=json.load(intents)

pd.set_option('display.max_colwidth', None)

#getting all the data to lists
tags = []
inputs = []
responses={}
for intent in data1['intents']:
  responses[intent['tag']]=intent['responses']
  for lines in intent['patterns']:
    inputs.append(lines)
    tags.append(intent['tag'])
#converting to dataframe
data = pd.DataFrame({"inputs":inputs,
                     "tags":tags})
data.head()

data

#encoding the outputs
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Sentiment = le.fit_transform(data['tags'])

data["label"]=Sentiment

data.head()

def clean(text):
  return text.lower()

data["inputs"]=data["inputs"].apply(clean)

data

import tensorflow
from tensorflow.keras.utils import to_categorical
import numpy as np
np.set_printoptions(threshold=np.inf)

y_train = to_categorical(data.label)

y_train.shape#



y_train


import transformers

from transformers import AutoTokenizer,TFBertModel
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
bert = TFBertModel.from_pretrained("bert-base-cased")

x_train = tokenizer(
    text=data.inputs.tolist(),
    add_special_tokens=True,
    max_length=20,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = False,
    return_attention_mask = True,
    verbose = True)

input_ids=x_train["input_ids"]

input_ids.shape



attention_mask=x_train["attention_mask"]

attention_mask.shape

y_train.shape

import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.initializers import TruncatedNormal
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import CategoricalAccuracy
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Input, Dense

max_len = 20
input_ids = Input(shape=(max_len,), dtype=tf.int32, name="input_ids")
input_mask = Input(shape=(max_len,), dtype=tf.int32, name="attention_mask")
embeddings = bert(input_ids,attention_mask = input_mask)[0] 
out = tf.keras.layers.GlobalMaxPool1D()(embeddings)
out = Dense(128, activation='relu')(out)
out = tf.keras.layers.Dropout(0.1)(out)
out = Dense(32,activation = 'relu')(out)
y = Dense(80,activation = 'sigmoid')(out)
model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)
model.layers[2].trainable = True

optimizer =  tf.keras.optimizers.legacy.Adam(
    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website 
    epsilon=1e-08,
    decay=0.01,
    clipnorm=1.0)
# Set loss and metrics
loss =CategoricalCrossentropy(from_logits = True)
metric = CategoricalAccuracy('balanced_accuracy'),
# Compile the model
model.compile(
    optimizer = optimizer,
    loss = loss, 
    metrics = metric)

train_history = model.fit(
    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,
    y = y_train,
    epochs=50,
    batch_size=36)



'''texts = input(str('input the text'))
x_val = tokenizer(
    text=texts,
    add_special_tokens=True,
    max_length=20,
    truncation=True,
    padding='max_length', 
    return_tensors='tf',
    return_token_type_ids = False,
    return_attention_mask = True,
    verbose = True) 
validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100

output=validation.argmax()

response_tag = le.inverse_transform([output])[0]

response_tag

import random

print("doctor Omdena: ",random.choice(responses[response_tag]))

#chatting
import string
import random
while True:
  text_p=[]
  texts = input("You:")
  texts = [letters.lower() for letters in texts if letters not in string.punctuation]
  texts = ''.join(texts)
  text_p.append(texts)


  x_val = tokenizer(
  text=text_p,
  add_special_tokens=True,
  max_length=20,
  truncation=True,
  padding='max_length', 
  return_tensors='tf',
  return_token_type_ids = False,
  return_attention_mask = True,
  verbose = True) 
  validation = model.predict({'input_ids':x_val['input_ids'],'attention_mask':x_val['attention_mask']})*100
  output=validation.argmax()
  response_tag = le.inverse_transform([output])[0]
  print("doctor Omdena: ",random.choice(responses[response_tag]))
  if response_tag == "goodbye":
    break'''

model.save("omdenatransformer.h5")



#importing the pretrained biobert tokenizer and the biobert model
from transformers import AutoTokenizer, TFAutoModel
biobert_tokenizer = AutoTokenizer.from_pretrained("cambridgeltl/BioRedditBERT-uncased")

biobert_model = TFAutoModel.from_pretrained("cambridgeltl/BioRedditBERT-uncased")

'''x_train = tokenizer(
    text=data.inputs.tolist(),
    add_special_tokens=True,
    max_length=20,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = False,
    return_attention_mask = True,
    verbose = True)'''

import pickle

# saving tokenizer
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    

# saving label encoder
with open('label_encoder.pickle', 'wb') as ecn_file:
    pickle.dump(le, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)